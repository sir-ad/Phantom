<!doctype html><html data-theme=dark lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><title>Index â€“ Phantom</title><meta content="The invisible force behind every great product" name=description><link href=https://phantom.pm/oat.min.css rel=stylesheet><link href=https://phantom.pm/docs.css rel=stylesheet><body><nav class=sidebar><a class=logo href=https://phantom.pm> <strong>Phantom</strong> </a><ul><li class=section-header>Technical Specs<li><a href=https://phantom.pm/docs/tech/architecture-overview/>Architecture Overview</a><li class=section-header>Providers<li><a href=https://phantom.pm/docs/providers/anthropic/>Anthropic</a><li><a href=https://phantom.pm/docs/providers/gemini/>Gemini</a><li><a href=https://phantom.pm/docs/providers/overview/>Index</a><li><a href=https://phantom.pm/docs/providers/ollama/>Ollama</a><li><a href=https://phantom.pm/docs/providers/openai/>Openai</a><li class=section-header>Features<li><a href=https://phantom.pm/docs/features/agents/>Agents</a><li><a href=https://phantom.pm/docs/features/chat/>Chat</a><li><a href=https://phantom.pm/docs/features/prd/>Prd</a><li><a href=https://phantom.pm/docs/features/simulation/>Simulation</a><li><a href=https://phantom.pm/docs/features/swarm/>Swarm</a><li class=section-header>User Guide<li><a href=https://phantom.pm/docs/user/install/>Install</a><li><a href=https://phantom.pm/docs/user/integrations/>Integrations</a><li><a href=https://phantom.pm/docs/user/modules/>Modules</a><li><a href=https://phantom.pm/docs/user/quickstart/>Quickstart</a><li><a href=https://phantom.pm/docs/user/troubleshooting/>Troubleshooting</a></ul></nav><main class=content><article><h1>Index</h1><hr><h2 id=sidebar-position-1title-ai-providers-overview>sidebar_position: 1 title: AI Providers Overview</h2><h1 id=ai-providers>AI Providers</h1><p>Phantom supports 4 LLM providers out of the box. You can use one or configure all of them â€” Phantom will auto-detect the best available provider.<h2 id=provider-comparison>Provider Comparison</h2><table><thead><tr><th>Provider<th>Models<th>Cost<th>Best For<th>Latency<tbody><tr><td><strong>Ollama</strong><td>Llama 3.1, Mistral, CodeLlama<td>ðŸŸ¢ Free<td>Privacy, offline, fast iteration<td>Low<tr><td><strong>OpenAI</strong><td>GPT-4o, GPT-4o-mini, o3-mini<td>ðŸ’³ Pay-per-use<td>Strategy, nuanced analysis<td>Medium<tr><td><strong>Anthropic</strong><td>Claude Sonnet 4, Opus, Haiku<td>ðŸ’³ Pay-per-use<td>Long-form writing, PRDs<td>Medium<tr><td><strong>Gemini</strong><td>2.0 Flash, 2.5 Pro, 1.5 Pro<td>ðŸ’³ Pay-per-use<td>Speed, multimodal<td>Low</table><h2 id=priority-order>Priority Order</h2><p>When you run <code>phantom</code> without specifying a provider, it checks in this order:<ol><li><strong>Ollama</strong> (if running locally)<li><strong>OpenAI</strong> (if <code>OPENAI_API_KEY</code> is set)<li><strong>Anthropic</strong> (if <code>ANTHROPIC_API_KEY</code> is set)<li><strong>Gemini</strong> (if <code>GEMINI_API_KEY</code> is set)</ol><h2 id=configuration>Configuration</h2><h3 id=environment-variables>Environment Variables</h3><pre><code data-lang=bash>export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="AIza..."
</code></pre><h3 id=config-file>Config File</h3><pre><code data-lang=bash>phantom config setup
</code></pre><p>This stores keys in <code>~/.phantom/config.json</code>.<h3 id=switching-providers>Switching Providers</h3><pre><code data-lang=bash># At launch
phantom chat --provider openai --model gpt-4o

# During chat
phantom (ollama:llama3.1) â–¸ /model gpt-4o
</code></pre><h2 id=fallback-chain>Fallback Chain</h2><p>If your primary provider fails (rate limit, network error), Phantom automatically falls back through the chain: OpenAI â†’ Anthropic â†’ Ollama â†’ Gemini.</article></main><script src=https://phantom.pm/oat.min.js></script>