<!doctype html><html data-theme=dark lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><title>Ollama – Phantom</title><meta content="The invisible force behind every great product" name=description><link href=https://sir-ad.github.io/Phantom/oat.min.css rel=stylesheet><link href=https://sir-ad.github.io/Phantom/docs.css rel=stylesheet><body><nav class=sidebar><a class=logo href=https://sir-ad.github.io/Phantom> <strong>▲ Phantom</strong> </a><ul><li><a href=https://sir-ad.github.io/Phantom>← Home</a><li><a href=https://sir-ad.github.io/Phantom/docs/install>Install</a><li><a href=https://sir-ad.github.io/Phantom/docs/modules>Modules</a></li><hr><li class=section-header>Technical Specs<li><a href=https://sir-ad.github.io/Phantom/docs/tech/architecture-overview/>Architecture Overview</a><li class=section-header>User Guide<li><a href=https://sir-ad.github.io/Phantom/docs/user/install/>Install</a><li><a href=https://sir-ad.github.io/Phantom/docs/user/integrations/>Integrations</a><li><a href=https://sir-ad.github.io/Phantom/docs/user/modules/>Modules</a><li><a href=https://sir-ad.github.io/Phantom/docs/user/quickstart/>Quickstart</a><li><a href=https://sir-ad.github.io/Phantom/docs/user/troubleshooting/>Troubleshooting</a><li class=section-header>Features<li><a href=https://sir-ad.github.io/Phantom/docs/features/agents/>Agents</a><li><a href=https://sir-ad.github.io/Phantom/docs/features/chat/>Chat</a><li><a href=https://sir-ad.github.io/Phantom/docs/features/analyze/>Deep Task Analysis</a><li><a href=https://sir-ad.github.io/Phantom/docs/features/oracle/>Phantom Oracle</a><li><a href=https://sir-ad.github.io/Phantom/docs/features/prd/>Prd</a><li><a href=https://sir-ad.github.io/Phantom/docs/features/simulation/>Simulation</a><li><a href=https://sir-ad.github.io/Phantom/docs/features/swarm/>Swarm</a><li class=section-header>Providers<li><a href=https://sir-ad.github.io/Phantom/docs/providers/anthropic/>Anthropic</a><li><a href=https://sir-ad.github.io/Phantom/docs/providers/gemini/>Gemini</a><li><a href=https://sir-ad.github.io/Phantom/docs/providers/overview/>Index</a><li><a href=https://sir-ad.github.io/Phantom/docs/providers/ollama/>Ollama</a><li><a href=https://sir-ad.github.io/Phantom/docs/providers/openai/>Openai</a></ul></nav><main class=content><article><h1>Ollama</h1><p><a href=https://ollama.com rel=external>Ollama</a> runs LLMs locally on your machine. It’s free, private, and fast.<h2 id=setup>Setup</h2><ol><li>Install Ollama from <a href=https://ollama.com rel=external>ollama.com</a>.<li>Pull a model:<pre><code data-lang=bash>ollama pull llama3.1:8b
</code></pre><li>Start Ollama:<pre><code data-lang=bash>ollama serve
</code></pre><li>That’s it — Phantom auto-detects Ollama at <code>localhost:11434</code>.</ol><h2 id=supported-models>Supported Models</h2><table><thead><tr><th>Model<th>Size<th>Best For<tbody><tr><td><code>llama3.1:8b</code><td>4.7 GB<td>General PM tasks, fast responses<tr><td><code>llama3.1:70b</code><td>40 GB<td>Deep analysis, complex strategy<tr><td><code>codellama:7b</code><td>3.8 GB<td>Technical specs, API design<tr><td><code>mistral:7b</code><td>4.1 GB<td>European language support, concise answers<tr><td><code>gemma2:9b</code><td>5.4 GB<td>Balanced quality and speed</table><h2 id=usage>Usage</h2><pre><code data-lang=bash>phantom chat --model ollama:llama3.1:8b
</code></pre><h2 id=configuration>Configuration</h2><p>Ollama defaults to <code>http://localhost:11434</code>. To use a remote Ollama instance:<pre><code data-lang=bash>phantom config set providers.ollama.baseUrl http://your-server:11434
</code></pre><h2 id=tips>Tips</h2><ul><li><strong>8B models</strong> are great for quick iteration and testing.<li><strong>70B models</strong> produce significantly better analysis but require 48+ GB RAM.<li>Run <code>ollama list</code> to see which models you have installed.</ul></article></main><script src=https://sir-ad.github.io/Phantom/oat.min.js></script>